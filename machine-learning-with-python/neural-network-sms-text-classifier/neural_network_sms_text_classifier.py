# -*- coding: utf-8 -*-
"""neural-network-sms-text-classifier.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kLfxgN-RoRhIdT0P_iy35BNfMLgT-aNQ
"""

# Import.
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense, Dropout, LSTM, Bidirectional
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.preprocessing.text import Tokenizer
import seaborn as sns
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import tensorflow as tf
from tensorflow import keras
import tensorflow_datasets as tfds

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt

# Download Data files.
!wget https: // cdn.freecodecamp.org/project-data/sms/train-data.tsv
!wget https: // cdn.freecodecamp.org/project-data/sms/valid-data.tsv

# Define Paths.
train_file_path = "train-data.tsv"
test_file_path = "valid-data.tsv"

# Define DataFrame.
df = pd.read_csv("train-data.tsv",
                 sep='\t',
                 header=None)
df.columns = ['label', 'message']

# Sneak peak of data.
df.describe()

# Categorize data in ham and spam.
ham_msg = df[df.label == 'ham']
spam_msg = df[df.label == 'spam']

# To numpy array. Needed to visualize using WordCloud.
ham_msg_numpy = " ".join(ham_msg.message.to_numpy().tolist())
spam_msg_numpy = " ".join(spam_msg.message.to_numpy().tolist())

# Visualize using WordCloud. For ham words.

ham_msg_cloud = WordCloud(width=512, height=512, stopwords=STOPWORDS, max_font_size=100,
                          background_color="black", colormap='Greens').generate(ham_msg_numpy)

plt.figure(figsize=(10, 10))
plt.imshow(ham_msg_cloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Visualize using WordCloud. For spam words.
spam_msg_cloud = WordCloud(width=512, height=512, stopwords=STOPWORDS, max_font_size=100,
                           background_color="White", colormap="magma").generate(spam_msg_numpy)

plt.figure(figsize=(10, 10))
plt.imshow(spam_msg_cloud, interpolation='bilinear')
plt.axis('off')
plt.show()

# Visualize using Seaborn.

plt.figure(figsize=(10, 10))
sns.countplot(df.label)

# Calculate percentage of spam messages.
spam_perc = (len(spam_msg)/len(ham_msg))*100
print(round(spam_perc), "% of the messages are spam.")

# Preprocessing
ham_msg_df = ham_msg.sample(n=len(spam_msg), random_state=0)
spam_msg_df = spam_msg

msg_df = ham_msg_df.append(spam_msg_df).reset_index(drop=True)

msg_df['text_length'] = msg_df['message'].apply(
    len)  # Calculate average length by label types
labels = msg_df.groupby('label').mean()

df_test = pd.read_csv("valid-data.tsv", sep='\t', header=None)
df_test.columns = ['label', 'message']

msg_df['msg_type'] = msg_df['label'].map({'ham': 0, 'spam': 1})
df_test['msg_type'] = df_test['label'].map({'ham': 0, 'spam': 1})

train_label = msg_df['msg_type']
train_msg = msg_df['message']
test_msg = df_test['message']
test_label = df_test['msg_type']

max_len = 50
trunc_type = "post"
padding_type = "post"
oov_tok = "<OOV>"
vocab_size = 500

tokenizer = Tokenizer(num_words=vocab_size,
                      char_level=False, oov_token=oov_tok)
tokenizer.fit_on_texts(train_msg)

word_index = tokenizer.word_index
tot_words = len(word_index)

training_sequences = tokenizer.texts_to_sequences(train_msg)
training_padded = pad_sequences(
    training_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)
testing_sequences = tokenizer.texts_to_sequences(test_msg)
testing_padded = pad_sequences(
    testing_sequences, maxlen=max_len, padding=padding_type, truncating=trunc_type)

# Dense Spam Detection Model.
vocab_size = 500
embeding_dim = 16
drop_value = 0.2
n_dense = 24
max_len = 50

# Design Model.

model = Sequential()
model.add(Embedding(vocab_size, embeding_dim, input_length=max_len))
model.add(GlobalAveragePooling1D())
model.add(Dense(24, activation='relu'))
model.add(Dropout(drop_value))
model.add(Dense(1, activation='sigmoid'))

model.summary()

# Compile the model.
model.compile(loss='binary_crossentropy',
              optimizer='adam', metrics=['accuracy'])

# Fit dense spam detector model.

num_epochs = 30
early_stop = EarlyStopping(monitor='val_loss', patience=3)
history = model.fit(training_padded, train_label, epochs=num_epochs, validation_data=(
    testing_padded, test_label), callbacks=[early_stop], verbose=2)

# Evaluate results
model.evaluate(testing_padded, test_label)

# TIME TO TEST

# function to predict messages based on model
# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])


def predict_message(pred_text1):
    pred_text = []
    pred_text.append(pred_text1)
    new_seq = tokenizer.texts_to_sequences(pred_text)
    padded = pad_sequences(new_seq, maxlen=max_len,
                           padding=padding_type, truncating=trunc_type)
    prediction = model.predict(padded)
    for i in prediction:
        if i > 0.5:
            return ((float(i), "spam"))
        else:
            return ((float(i), "ham"))


pred_text = " you have won £1000 cash! call to claim"

prediction = predict_message(pred_text)
print(prediction)

# Run this cell to test your function and model. Do not modify contents.


def test_predictions():
    test_messages = ["how are you doing today",
                     "sale today! to stop texts call 98912460324",
                     "i dont want to go. can we try it a different day? available sat",
                     "our new mobile video service is live. just install on your phone to start watching.",
                     "you have won £1000 cash! call to claim your prize.",
                     "i'll bring it tomorrow. don't forget the milk.",
                     "wow, is your arm alright. that happened to me one time too"
                     ]

    test_answers = ["ham", "spam", "ham", "spam", "spam", "ham", "ham"]
    passed = True

    for msg, ans in zip(test_messages, test_answers):
        prediction = predict_message(msg)
        if prediction[1] != ans:
            passed = False

    if passed:
        print("You passed the challenge. Great job!")
    else:
        print("You haven't passed yet. Keep trying.")


test_predictions()
